[
  {
    "objectID": "fastStat.html",
    "href": "fastStat.html",
    "title": "All of REAL Statistics",
    "section": "",
    "text": "Author bio\n(Above image source unknown. See notice at the end of this document regarding copyright of the document.)"
  },
  {
    "objectID": "fastStat.html#lesson-sampling-the-notion-of-a-sample",
    "href": "fastStat.html#lesson-sampling-the-notion-of-a-sample",
    "title": "All of REAL Statistics",
    "section": "Lesson SAMPLING: the Notion of a Sample",
    "text": "Lesson SAMPLING: the Notion of a Sample\nWe’ve all heard the term margin of error in an opinion poll. It will be discussed in detail in a later lesson, but what question is it addressing?\nSay the poll consists of querying 1200 people, which is common in polls. These were drawn randomly from some list, say a list of phone numbers. We ask each one, “Do you favor Candidate A?” The point is that if we were to do this again, we would get 1200 other people, and the percentage saying Yes to our question would change. Thus we want to have some idea as to how much our Yes percentage varies from one sample of 1200 people to another.\nSampling 1200 people is not referred to as “1200 samples.” The set of 1200 people is referred to as one sample of size 1200.\nLet’s set some notation. Say we are interested in some quantity X, say human height. We take a sample of n people from a given target population, and denote the value of X in the ith person in our sample by Xi. If we sample with replacement (or if n is small relative to the total population size), the Xi are independent random variables. Also, each Xi has distribution equal to that of the sampled population. If, say 22.8% of people in this population are taller than 70 inches, then P(Xi &gt; 70) = 0.228.\nSo, X1,…,Xn are independent, identically distributed random variables (iid)."
  },
  {
    "objectID": "fastStat.html#lesson-normaletc-the-role-of-normal-gaussian-and-other-parametric-distribution-families",
    "href": "fastStat.html#lesson-normaletc-the-role-of-normal-gaussian-and-other-parametric-distribution-families",
    "title": "All of REAL Statistics",
    "section": "Lesson NORMALETC: the Role of Normal (Gaussian) and Other Parametric Distribution Families",
    "text": "Lesson NORMALETC: the Role of Normal (Gaussian) and Other Parametric Distribution Families\nIn the last lesson, we talked about the distribution of X in the population. Although the population is finite (more on this below) and thus X is a discrete random variable, one often models X as continuous, with its distribution being in the normal family.\nWhy do this?\n\nHistograms of X in many applications do look rather bell-shaped. This may in turn be due to the Central Limit Theorem (CLT). The CLT says that sums are approximately normal, and in the human height case, one can think of the body as consisting of chunks whose heights sum to the height of the person. (The CLT assumes i.i.d. summands, and the chunks here would be neither independent nor indentically distributed, but there are non-i.i.d. versions of the CLT.)\nThe early developers of statistics had no computers, and it turns out that the normal distribution family is quite mathematically tractable, thus amenable to closed-form “exact” solutions.\nIt is often the case in math that discrete quantities are approximated by continuous ones (also vice versa).\nA normal distribution is determined by two parameters, the mean and variance of the distribution. Without that assumption, we have many parameters, essentially infinitely many. Let Fx be the cdf of X, i.e. FX(t) = P(X ≤ t). Well, there are infinitely many possible values for t, thus infinitely many values of FX(t). But if we assume X is normal, those infinitely many values are all expressible in terms of just two numbers. We are then essentially estimating two numbers instead of infinitely many. This is often a very useful approximation.\n\nAnother popular model is the exponential distribution family. You probably learned in your probability course that it is “memoryless,” which makes it a suitable model in some applications.\nNote that, as models, these are necessarily inexact. No distribution in practice is exactly normal, for instance. No one is 900 feet tall, and no one has a negative height. For that matter, a normal distribution is continuous, whereas X is discrete, for two reasons:\n\nWe are sampling from a finite population.\nOur measuring instruments have only finite precision. If, say, X is bounded between 0 and 10, and is measured to 2 decimal places, X can take on 1000 values, and thus is discrete.\n\nBut what are we estimating, in light of the fact that our model is only approximate? Say for instance we model X as having a gamma distribution. Then in some sense, depending on how we estimate, we are estimating the gamma distribution that is closest to our true population distribution."
  },
  {
    "objectID": "fastStat.html#lesson-conceptpops-conceptual-populations",
    "href": "fastStat.html#lesson-conceptpops-conceptual-populations",
    "title": "All of REAL Statistics",
    "section": "Lesson CONCEPTPOPS: Conceptual Populations",
    "text": "Lesson CONCEPTPOPS: Conceptual Populations\nIn the opinion poll example, it is clear as to which population is sampled. In many applications, the issue is more conceptual. If for instance we run a clinical trial with 100 diabetic patients, we might think of them as having been sampled from the population of all diabetics, even though we did not actually select the patients in our sample, whether randomly or otherwise.\nThis issue can become quite a challenge in, say, economic analysis. If we have 10 years of annual data, i.e. n = 10, what population is that a “sample” from?\nAccordingly, in many applications, the population we model as being sampled from is largely conceptual rather than a tangible entity.\nSay we have 1200 patients in a clinical trial of a drug for treating hypertension. Then our sample might be considered as a random sample of the population of all sufferers of hypertension, but this may be an oversimplification. Serious consideration should be given to the implications of our sampling method on the definition of the sampled population."
  },
  {
    "objectID": "fastStat.html#lesson-stderrs-standard-errors",
    "href": "fastStat.html#lesson-stderrs-standard-errors",
    "title": "All of REAL Statistics",
    "section": "Lesson STDERRS: Standard Errors",
    "text": "Lesson STDERRS: Standard Errors\nEarlier we mentioned the “margin of error” in reporting the results of opinion polls. To make that notion concrete, let’s first discuss a related idea, standard errors.\nWe use our data X1,…,Xn to estimate some quantity of interest, say the proportion q of people in the population who would answer Yes to our poll if we had a chance to ask them all. Our estimate, Q, would be the proportion of people in our sample who say Yes.\nExtremely important note: Make sure to always carefully distinguish between a population quantity, q in this case, and its sample estimate, Q here.\nWe want to have some measure of how much Q varies from one sample to another. Of course, Var(Q) is such a measure.\nSay for now that the average of Q, averaged over all possible samples, is q. For some samples, Q &gt; q, for others Q &lt;- q, but on average we get q. This relates to the issue of bias, which we will turn to later, but for now, say we have this situation, i.e. EQ = q.\nThe key point: If Var(Q) is small, then Q doesn’t vary much from one sample to another, and if EQ = q, then for “most” samples, Q should be near q. That is exactly what we hope for! We only have one sample, of course, but if we know that Q is usually near q, we feel reasonably confident that the q from our particular sample is near q.\nOf course, the square root of any variance is called the standard deviation. In the case of an estimator, Q here, we use the term standard error. In some cases, it will be only the approximate standard deviation, in a sense to be seen later."
  },
  {
    "objectID": "fastStat.html#lesson-bias-bias-and-impact-on-standard-errors",
    "href": "fastStat.html#lesson-bias-bias-and-impact-on-standard-errors",
    "title": "All of REAL Statistics",
    "section": "Lesson BIAS: Bias, and Impact on Standard Errors",
    "text": "Lesson BIAS: Bias, and Impact on Standard Errors\nIn our last lesson, we assumed that EQ = q. We say that Q is an unbiased estimator of q. In English: The average value of Q over all possible samples is q.\nIn the above example, in which Q is the sample proportion of Yes’s and q is the correspondng population proportion, it does turn out that Q is unbiased. In fact, any sample mean is an unbiased estimator for the population mean.  Let’s skip the derivation for now (we’ll have a derivations lesson later), so we can get to the larger issues.Here X is 1 or 0, with 1 meaning the respondent answered Yes, and 0 meaning No. So the average of the Xi, Q, is the proportion of voters choosing Candidate A.\nUnbiasedness at first seems to be a very desirable property. It does hold for some classical statistical methods, but does not hold for most others, one of which is the sample variance, as follows.\nSay we wish to estimate the population variance σ2. Note that\n\\[\nVar(X_{i}) = \\sigma^2\n\\]\nAnd recall that also\n\\[\nEX_{i} = \\mu\n\\]\nthe population mean. Note too that we are not necessarily assuming a normal distribution.\nIn the population, that is the average squared difference between the data and their mean. The sample analog is\n\\[\nS^{2} = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})^2\n\\]\nwhere \\(\\bar{X}\\) is the sample mean,\n\\[\n\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\n\\]\nNote especially the word analog above. Here is the analogy:\n\nσ2 is the average squared distance of X, in the population, to the population mean:\n\n\\[\n\\sigma^2 = E[(X - \\mu)^2]\n\\]\n\nS2 is the average squared distance of X, in the sample, to the sample mean.\n\nIt can be shown that S2 is biased downward:\nE(S2) = [(n-1) / n] σ2\nThe average value of S2 over all samples is a little too low. The amount of bias is\nE(S2) - σ2 = -1/n σ2\nThis is usually tiny, but it bothered the early developers of statistics, who then adjusted the definition of sample variance to\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2\n\\]\nBy the way, in the field of probability and statistics, it is customary to use capital letters for random variables. This is an exception.\nNote our distinction between S2 and the similar but slightly different quantity, \\(s^2\\). Both are random variables – they depend on the Xi, which are random – but they are different random variables. Typographical case matters.\nSince for any random variable W (for which EW exists) and any constant c we have E(cW) = c EW, s2 is now unbiased.\nThe variance of \\(\\bar{X}\\) turns out to be \\(\\sigma^2/n\\). But we don’t know \\(\\sigma^2\\), just like we don’t know \\(\\mu\\). So we estimate \\(\\sigma\\) by S or s. So, we set\n\\[\ns.e.(\\bar{X}) = S / \\sqrt{n}\n\\]\n(use s instead of S if you wish; it won’t matter below).\nBut most estimators are not only biased, but also lack simple adjustments like that for S2 above. So, one must accept bias in general, and consider its implications.\nNow, should we use S or s? Say an estimator has variance of size \\(O(\\frac{1}{n})\\), as above. Yet the difference in expected values of S and s is of size \\(O(\\frac{1}{n^2})\\). In other words, it really doesn’t matter whether we use S or s for larger samples; the bias is small relative to the standard error, so the argument in the last lesson still holds."
  },
  {
    "objectID": "fastStat.html#lesson-ci-confidence-intervals",
    "href": "fastStat.html#lesson-ci-confidence-intervals",
    "title": "All of REAL Statistics",
    "section": "Lesson CI: Confidence Intervals",
    "text": "Lesson CI: Confidence Intervals\nIn our opinion poll example, Q is called a point estimate of q. We would also like to have an interval estimate, which gives a range of values. If say in in an election, the results of an opinion poll are reported as, “Candidate X has support of 62.1% of the voters, with a margin of error of 3.9%,” it is saying,\n\nA 95% confidence interval (CI) for X’s support is (58.2%,66.0%).\n\nWe will show below that the radius of a 95% CI is 1.96 times the standard error of the given estimator Q. By the way, most CIs in this document will be only approximate, a point to be discussed shortly.\nA key point is the meaning of “95% confident.” Imagine forming this interval on each possible sample from the given population. Then 95% of the intervals would cover the true value, q.\nA note on the phrasing “q is in our interval”:\n\nSome may take this to mean that q is random, which it is not; q is unknown but fixed. The CI is what varies from one sample to another. Some instructors are so worried about such misinterpreation that they ban the phrasing “q is in the CI,” insisting that students say “The CI contains q, to emphasize that the CI is random, not q. To me, that’s going to absurd lengths to make a point – the two statements are linquistically equivalent, after all – but again, these instructors feel that misintepretation is less likely this way.\n\nA related, and much deeper, philosophical question is whether it is correct to say, “The probability that q is in this CI is 0.95.” Most statisticians would say that tnis too is an incorrect. They say it violates the view that the 95% figure can only be applied in the “repeated sampling” context. To them, the probability that q is in this CI is either 0 or 1.\nBut really, what do we mean by probability? It depends on the context. Consider the famous Monty Hall game show problem. To the host, the probability is 1 that the car is behnd door 3. But to the contestant, that probability is 2/3. In the CI situation, “Nature” knows whether this CI contains q; to us, the probability is 0.95."
  },
  {
    "objectID": "fastStat.html#lesson-ciapprox-confidence-intervals-from-asymptotics",
    "href": "fastStat.html#lesson-ciapprox-confidence-intervals-from-asymptotics",
    "title": "All of REAL Statistics",
    "section": "Lesson CIAPPROX: Confidence Intervals from Asymptotics",
    "text": "Lesson CIAPPROX: Confidence Intervals from Asymptotics\nThe early developers of statistics defined a distribution family known as Student’s t. This supposedly can be used to form “exact” CIs, i.e. the probability of the CI covering the target population value is exactly 0.95, if the population distribution of X is normal. Student-t is widely taught, and thus widely used. BUT it is just an illusion. As pointed out earlier, no distribution in practice is exactly normal.\nWhat saves the day, though, is the Central Limit Theorem. \\(\\bar{X}\\) is a (scaled) sum, which the CLT tells us is asymptotically normally distributed as the sample size n goes to infinity.\nTechnically, the CLT is phrased only in terms of cumulative distribution functions, not probability density functions. So this comment on a histogram doesn’t follow from the CLT, but “the probabilities for any range of values” does follow from it.j More on this below, but note that there is also the Local Limit Theorem for density convergence, and in practice, the density functions converge too.\nIn other words, if we were to compute \\(\\bar{X}\\) on each of the possible samples of size n from the population, and then plot the results in a histogram, the graph would be approximately bell-shaped, and the probabilities for any range of values would be approximately those of a normal distribution. Moreover, the large n, the more bell-shaped it would be.\nAh, so we’re in business: For any random variable W, the quantity\n(W - EW) / (Var(W)0.5)\nhas mean 0 and variance 1. (This stems simply from the properties of mean and variance; it has nothing to do with whether W is normal or not.) So,\n\\[\nZ = \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\n\\]\nhas mean 0 and variance 1, where μ is the population mean of X. (Recall that \\(\\bar{X}\\) is unbiased for \\(\\mu\\).)\nAnd since Z actually does have an approximately normal distribution, its distribution is thus approximately N(0,1), i.e. normal with mean 0 and variance 1. Replacing σ, which is unknown. by its estimate s can be further justified mathematically, so let’s redefine Z as\n\\[\nZ = \\frac{\\bar{X} - \\mu}{\\frac{s}{\\sqrt{n}}}\n\\]\nwhich is approximately N(0,1), and this is what we exploit to form a CI, as folows.\nThe N(0,1) distribution has 95% of its area between -1.96 and 1.96: e.g. run\nqnorm(0.025)\nSo we have\n\\[\n0.95 \\approx P(-1.96 &lt; \\frac{\\bar{X} - \\mu}{s/\\sqrt{n}} &lt; 1.96)\n\\]\nwhich after algebra becomes\n\\[\n0.95 \\approx P[\\bar{X} - 1.96 ~ s.e.(\\bar{X}) &lt; \\mu &lt;\n\\bar{X} + 1.96 ~ s.e.(\\bar{X})]\n\\]\nThere’s our CI for μ!\n\\[\n(\\bar{X} - 1.96 ~ s.e.(\\bar{X}),\n\\bar{X} + 1.96 ~ s.e.(\\bar{X}))\n\\]\nWe are (approximately) 95% confident that the true population mean is in that interval. (Remember. it is the interval that is random, not μ.)\nAnd things don’t stop there. Actually, many types of estimators have some kinds of sums within them, and thus have an approximately normal distribution, provided the estimator is a smooth function of those sums.\n(Smooth here means a smooth curve, no sharp corners, thus having a derivative. A Taylor series approximation results in a linear function of normal random variable, thus again normal.)\nThus approximate CIs can be found for lots of different estimators, notably Maximum Likelihood Estimators and least-squares parametric regression estimators, as we will see in later lessons.\nIn other words, we have the following principle:\nThe Fundamental Tool of Statistical Inference:\n(The name here is my own, not a general term, but the principle is widely used.)\n\nIf R is a “smooth” estimator of a population quantity r, and R consists of sums, based on an i.i.d. sample, then an approximate 95% confidence interval for r is\n(R - 1.96 s.e.(R), R + 1.96 s.e.(R))\n\nAgain, the term smooth roughly means that R is a differentiable function of X1, …, Xn.\nBy the way, for large n, the Student-t distribution is almost identical to N(0,1), so “no harm, no foul” – Student-t will be approximately correct. But it won’t be exactly correct, in spite of the claim."
  },
  {
    "objectID": "fastStat.html#lesson-indicators-cis-from-indicator-variables",
    "href": "fastStat.html#lesson-indicators-cis-from-indicator-variables",
    "title": "All of REAL Statistics",
    "section": "Lesson INDICATORS: CIs from Indicator Variables",
    "text": "Lesson INDICATORS: CIs from Indicator Variables\nOften X has only the values only 1 and 0, indicating the presence or absence of some trait. That is the case in the opinion poll, for example, where the respondent replies Yes (1) or not-Yes (0). Such a variable is called an indicator variable, as it indicates whether the trait is present or not. It is also called dummy variable and in the case of machine learning people, a one-hot variable.\nIn this case, Ā is the average of a bunch of 0s and 1s. The sum of such numbers will be the number of 1s, so Ā reduces to the proportion of 1s. The quantity μ is then the population proportion P(X = 1).\nIn the election poll example, Ā is then Q, the proportion of Yes responses to the opinion poll, with q being the corresponding population proportion.\nAfter some algebraic simplification, it turns out that\nS2 = Ā (1-Ā) / n\n(or use n-1 instead of n for s).\nOur earlier CI for a population mean now becomes in this special case\n(Ā - 1.96 sqrt[ Ā (1-Ā) / n], Ā + 1.96 sqrt[ Ā (1-Ā) / n])"
  },
  {
    "objectID": "fastStat.html#lesson-geyser-old-faithful-geyser-example",
    "href": "fastStat.html#lesson-geyser-old-faithful-geyser-example",
    "title": "All of REAL Statistics",
    "section": "Lesson GEYSER: Old Faithful Geyser Example",
    "text": "Lesson GEYSER: Old Faithful Geyser Example\nTo start to make the concepts tangible, let’s look at faithful, a built-in dataset in R, recording eruptions of the Old Faithful Geyser in the US National Park, Yellowstone.\nThe object consists of an R data frame with 2 columns, eruptions and waiting, showing the eruption durations and times between eruptions. the conceptual “population” here is the set of all Old Faithful eruptions, past, present and future.\nLet’s start simple, with a CI for the population mean duration:\nerps &lt;- faithful$eruptions\nsamplemean &lt;- mean(erps)\ns2 &lt;- var(erps)  # this is s^2 not S^2\nstderr &lt;- sqrt(s2/length(erps))\nc(samplemean - 1.96*stderr, samplemean + 1.96*stderr)\n# (3.35,3.62)"
  },
  {
    "objectID": "fastStat.html#lesson-converge-more-on-asymptotics",
    "href": "fastStat.html#lesson-converge-more-on-asymptotics",
    "title": "All of REAL Statistics",
    "section": "Lesson CONVERGE: More on Asymptotics",
    "text": "Lesson CONVERGE: More on Asymptotics\nThe concept of standard error needs to be clarified.\nWhat does the CLT say, exactly? The formal statement is\n\nSay Sn is the sum of iid random variables, each with mean μ and variance σ2. Then\nZn = (Sn - μ) / [σ sqrt(n)]\nconverges in distribution to N(0,1), meaning that the cdf of Zn converges to the N(0,1) cdf.\n\nIn other words, the asymptotics apply to probabilities–but not to expected values etc. Two random variables can have almost the same cdf but have very different means, variances and so on. E.g. take any random variable and shift a small amount of its probability mass from X = c to some huge number X = d; almost the same cdf, very different mean.\nAs noted, if a sum is an ingredient in some complicated function that produces our estimator, we can apply a Taylor series argument to say that the estimator is asymptotically normal. But the standard deviation of that normal distribution may be different from that of the estimator itself, due to the inaccuracy of the Taylor approximation.\nSo, the asymptotic statement about regarding R and r above enables us to compute asymptotically valid CIs based on the N(0,1) cdf. But the standard error used in that interval, s.e.(R), is not necessarily the standard deviation of R."
  },
  {
    "objectID": "fastStat.html#lesson-somemath-some-derivations",
    "href": "fastStat.html#lesson-somemath-some-derivations",
    "title": "All of REAL Statistics",
    "section": "Lesson SOMEMATH: Some Derivations",
    "text": "Lesson SOMEMATH: Some Derivations\nAs noted earlier, the goal of this tutorial is to develop within the reader an understanding of the intuition underlying statistical concepts or methods. It is not a tutorial on math stat. Nevertheless, it’s important to show a few derivations.\nĀ is an unbiased estimator of μ:\nBy the linearity of E(),\nEĀ = (1/n) Σin Exi = (1/n) n μ = μ\nVar(Ā) = (1/n) σ2, where σ2 is the population variance of X\nBy the fact that the variance of a sum of independent random variables is the sum of their variances,\nVar(Ā) = (1/n2) Σin Var(xi) = (1/n2) n σ2 = (1/n) σ2\nS2 is a biased estimator of σ2\nFor any random variable W (with finite variance),\nVar(W) = E(W2) - (EW)2\nWe will use this fact multiple times.\nAnd the sample analog (just algebraic manipulation) is\nS2 = (1/n) Σin Xi2 - Ā2\nApplying E() to both sides of this last equation, we have\nE(S2) = (1/n) n E(X2) - (Var(Ā) + μ2) = (σ2 + μ2) - [(1/n) σ2 + μ2] = [(n-1)/n] σ2"
  },
  {
    "objectID": "fastStat.html#lesson-sig-significance-testing",
    "href": "fastStat.html#lesson-sig-significance-testing",
    "title": "All of REAL Statistics",
    "section": "Lesson SIG: Significance Testing",
    "text": "Lesson SIG: Significance Testing\nThe notion of signficance testing (ST) (also known as hypothesis testing, and p-values) is one of the real oddities in statistics.\n\nOn the one hand, statisticians are well aware of the fact that ST can be highly miseadling.\nBut on the other hand, they teach ST with little or no warning about its dangers. As a result, its use is widespread–or more accurately stated, entrenched.\n\nAs mentioned, the problems of ST have always been well-known, but nothing was done about them. Finally, in 2016, the American Statistical Association released its first-ever position paper on any topic, stating what everyone had known for decades: ST is just not a good tool.\nTo be sure, the ASA stopped short of recommending fully against ST, as some on the committee were defenders of it to some extent. The final statement did vaguely say ST is useful in some settings. But they gave no examples of this, and at the very least agreed that ST is indeed widely misused. But it is my position that ST should simply not be used at all. Instead, analysis should be based on CIs, as explained later in this lesson.\nBut first, what is ST? To keep things simple, let’s say we have some coin, and want to test the null hypothesis H0: r = 0.5, where r is the probability of heads. We will toss the coin n times, and set R to the proportion of heads in our sample.\nWe take an “innocent until proven guilty” approach, clinging to our belief in H0 until/unless we see very strong evidence to the contrary. Well, what constitutes “strong”?\nWe look at the ratio W = (R-0.5) / s.e.(R). R is approximately normal, as noted earlier, and under H0 R has mean 0. So under H0, W ~ N(0,1). (Tilde is standard notation for “distributed as.”) Then P(|W| &gt; 1.96) ≈ 0.05 and 5% is the traditional standard for “strong evidence.” We reject H0 if and only if |W| &gt; 1.96.\nAnd we can get greedy. What if, say, we find that R = 2.2? Note:\n&gt; pnorm(-2.2)\n[1] 0.01390345\nUnder H0, P(|W| &gt; 2.2) ≈ 0.028. Ah, we would have rejected H0 even under the more stringent “strong evidence” criterion of 0.028. So we report 0.028, known as the p-value. The smaller the p-value, the more signficant we declare our finding.\nWell, what’s wrong with that?\n\nWe know a priori that H0 is false. No coin is exactly balanced. So it’s rather silly to ask the question regarding H0.\nWe might be interested in knowing whether the coin is approximately balanced. Fine, but the ST is not addressing that question. Even if r is just a little different from 0, then as the number of tosses n goes to infinity, the denominator in W goes to 0, and thus R goes to ±∞–and the p-value goes to 0.\nIn such a scenario, we declare that the coin is “signficantly” unbalanced (“Look, a tiny p-value!”), an egregiously misleading statement.\nWe have the opposite problem with small n. We will declare “No signficant difference” when we actually should say, “We have too little data to make any claims.”\n\nOne reason that ST is so appealing is that it allows the user to be lazy. Say we have two drugs, an old one A and a new one B, for treating high blood pressure. The user here may be a government agency, deciding whether to approve the new drug.\nLet μa and μb denote the population mean effects of the two drugs. Then the null hypothesis is\nH0: μa = μb\nAgain, we know a priori that H0 must be false–the two means can’t be equal, to infinitely many decimal places–so the test is meaningless in the first place, and we will have the problems described above. But, forget all that–the test gives us an answer, and the user is happy.\nA more careful analysis would be based on forming a CI for the difference μa - μb. That gives us a range of values, against which we can weigh things like cost differences between the two drugs, possible side effects and so on. In addition, the width of the interval gives the user an idea as to whether there is enough data for accurate estimation of the means.\nYes, the user does have to make a decision, but it is an informed decision. In other words:\n\nThe user is taking responsibility, making an informed decision, instead of allowing a poor statistical procedure to make the decision for him/her.\n\nNote by the way that what we are NOT doing is “check to see whether the CI contains 0,” which would have all the problems cited above.\nSee this document for further details."
  },
  {
    "objectID": "fastStat.html#lesson-estdistrs-estimating-entire-distributions",
    "href": "fastStat.html#lesson-estdistrs-estimating-entire-distributions",
    "title": "All of REAL Statistics",
    "section": "Lesson ESTDISTRS: estimating entire distributions",
    "text": "Lesson ESTDISTRS: estimating entire distributions\nRecall the cumulative distribution function` (cdf) of a random variable\nfX(t) = P(X ≤ t)\nThis is indeed a function; for different values of t, we get different values of the cdf.\nAnd as always, there are population and sample estimates. If X is human height, then FX(66.5) is the population proportion of people with height at most 66.5 inches; the sample estimate of that quantity is the corresponding sample proportion.\nThe entire sample estimate of FX is called the empirical cdf (ECDF). Let’s plot it for the geyser data we saw in an earlier lesson.\nplot(ecdf(erps))\n\n\n\nalt text\n\n\nSince the ECDF consists of proportions (one for each t), it is unbiased. For each t, E[ECDF(t)] =FX&lt;/sub(t).\nWe next turn to estimating density functions. We will spend quite a bit of time on this topic, as it illustrates key points that extend to statistics in general."
  },
  {
    "objectID": "fastStat.html#lesson-dens1-estimating-probability-density-functionshistograms",
    "href": "fastStat.html#lesson-dens1-estimating-probability-density-functionshistograms",
    "title": "All of REAL Statistics",
    "section": "Lesson DENS1: estimating probability density functions–histograms",
    "text": "Lesson DENS1: estimating probability density functions–histograms\nWhat about estimating the probability density function (pdf)? As noted in Lesson NORMALETC, pdfs do not really exist in practice. Technically, all random variables in real life are discrete rather than continuous, e.g. because of the finite precision of our measuring instruments.\nThus phrasing like “the” pdf above really means the pdf that best approximates the true population distribution. But from here on, let’s just speak of estimating “the” pdf.\nHistograms are pdf estimators:\nActually, the familiar histogram is a pdf estimator, provided we specify that the total area under it is 1.0. Here’s why:\nFirst, consider a histogram for the geyser data:\nhist(erps,probability=TRUE)  # second argument indicates area = 1.0\n\n\n\nalt text\n\n\nHistograms involve partitioning the range of the data into a certain number of bins (which can be specified with the breaks argument in hist()).\nLet’s look a little closer:\n&gt; u &lt;- hist(erps,probability=TRUE)\n&gt; u\n$breaks\n[1] 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5\n\n$counts\n[1] 55 37  5  9 34 75 54  3\n...\nHere hist() gave us the default bin boundaries for this data, at 1.5, 2.0, 2.5 and so on. We see that for instance 37 values of the erps fell into the interval from 2.0 to 2.5. This 37 figure was then used to determine the height of the plot for this interval. Why?\nRecall that the pdf of X, denoted by fX, is the derivative of the cdf FX. Recall too that a derivative is the slope of the tangent line to a curve. This in turn is a limit of slopes of line segments (secant lines) between two points on the curve.\nHere is an illustration. We drew the N(0,0.5) cdf, and then the tangent line at x = 0.5, as well as the secant line from x = 0.5 to x = 1.0.\n\n\n\nalt text\n\n\nHere is the code:\ncurve(pnorm(x,0,0.5),-2.5,2.5)\n# draw secant line from x = 0.5 to x = 1\npoint1 &lt;- c(0.5,pnorm(0.5,0,0.5))\npoint2 &lt;- c(1.0,pnorm(1.0,0,0.5))\npoint12 &lt;- rbind(point1,point2)\nlines(point12[,1],point12[,2])\n# abline() slope s at (q,r)\nabscd &lt;- function(q,r,s) abline(r-s*q,s)\nabscd(0.5,pnorm(0.5,0,0.5),dnorm(0.5,0,0.5))\nAs you can see, the secant slope is considerably less than the tangent slope. But if we were to draw the secant from x = 0.5 to say, x = 0.6, the tangent and secant slopes would be more similar.\nSo, for an interval (a,b), with b-a small,\nfX(a) ≈ [FX(b) - FX(a)] / (b-a)\nFX(b) - FX(a) is the probability that X is in (a,b], which we can estimate from the data, say for a = 2.0 and b = 2.5:\n&gt; sum(2.0 &lt; erps & erps &lt;= 2.5)\n[1] 37\n&gt; sum(2.0 &lt; erps & erps &lt;= 2.5) / length(erps)\n[1] 0.1360294\nHere (b-a) = 2.5 - 2.0 = 0.5. So,\nfX(2.0) ≈ 0.1360294 / 0.5 = 0.2720588\nSo, it makes sense that our histogram, viewed as a density estimate, used that 37 counts value. The rest, i.e. dividing by the total number of data points 272 times the interval width 0.5, just enter in to make the total area 1.0, which a density must have. As we saw above, we can request this latter property via the probability argument:\nIt’s only approximate!\nNote carefully that the symbol ≈ above stems from two considerations:\n\nWe are approximating a tangent by a secant.\nWe are using an estimate of FX from our sample data, not the population. We return to this point shortly.\n\n&gt; hist(erps,probability=TRUE)\nBut we’re not done. Why have an interval width (bin width) of 0.5? It came as the hist() default (which actually set the number of intervals). Maybe the intervals should be shorter? Longer? We have this tradeoff:\n\nIf we make b-a too small, we won’t have enough data to get a good estimate.\nIf we make b-a too large, then the approximation\nfX(a) ≈ [FX(b) - FX(a)] / (b-a)\nwill not be accurate.\n\nWe will return to this point later, in Lesson TRADE.\nKeeping this approximate nature in mind, it is interesting to note that our histogram here is bimodal, i.e. has two peaks. There have been many geophysical theories on this, e.g. postulating that there are two kinds of eruptions.\nHistograms are considered rather crude estimators of a density, as they are so choppy. A more advanced approach is that of kernel density estimators.\nIn estimating the density at a given point t, kernel estimators work by placing more weight on the data points that are closer to t, with the weights being smooth functions of the distance to t. A smooth curve results.\nBut what does “close” mean? Just as histograms have a tuning parameter (called a hyperparameter in machine learning circles) in the form of the bin width, kernel estimators have something called the bandwidth. Let’s not go into the formula here, but the point is that smaller bandwidths yield a more peaked graph, while larger values produce flatter curves.\nHere are the graphs for bandwidths of 0.25 and 0.75:\nplot(density(erps,bw=0.25))\nplot(density(erps,bw=0.75))\n\n\n\nalt text\n\n\n\n\n\nalt text\n\n\nThe first graph seems to clearly show 2 bells, but the second one is rather ambiguous. With even larger values for the bandwidth (try it yourself), we see a single bell, no hint of 2.\nHow should we choose the bandwidth, or for that matter, the bin width? if we make the bandwidth too large, some important “bumps” may not be visible. On the other hand, if we make it too small, this will just expose sampling variability, thus displaying “false” bumps.\nWe’ll address this (but unfortunately not answer it) next."
  },
  {
    "objectID": "old/StatBareMin.html",
    "href": "old/StatBareMin.html",
    "title": "Statistics–the Bare Minimum (or even less)",
    "section": "",
    "text": "This document presents a very brief overview of the main ideas in practical statistics. The only background assumed is high school algebra, e.g. familiarity with the slope of a line. We also assume that the reader has been exposed (superficially) to the notions of mean, standard deviation and histograms.\nOccasionally there are references to data frames etc. for readers who use R or Python for data science, but these references are not essential to the narrative. Near the end of the document, examples of the concepts that use actual R statistical operations are shown, but this is not intended as a tutorial on R, and can be understood with knowing the R details. (See my fasteR tutorial for a quick introduction to R.)\nIn addition there are also references to my fastStat tutorial, which is more detailed and presumes more background (a calculus-based probability course) on the part of the reader. These may be safely ignored by readers without this background.\n\n\nMain topics:\n\ncentral importance of sampling variation\nstandard errors; statistical inference\nprediction, especially with linear models\neffect estimation, confounding\n\nNotably missing:\n\nnormal, chi-square, F distributions\ninordinate emphasis of hypothesis testing and p-values\n\n\n\n\nSay we have data on height, age and weight for 100 people. Then the fact that we have 3 variables is denoted p = 3. The number of people, 100 here, is denoted n = 100.\nOf course, data points need not be people. For instance, we may have data on a sample of n = 25 elementary schools in a US state, with the data on any school being the number of students and number of teachers. We’d have p = 2 here.\nBy the way, the proper statistical phrasing is that, say in the school example, is “We have a sample of size 25,” not “We have 25 samples.”\nWe’ll use X1 to denote the first data point in our sample. In the people example above, X1 denotes the height, age and weight of the first person in our sample. X2 is our second data point, say the numbers of students and teachers for the second school in the school example. X3, X4 and so on are defined similarly.\nStorage in a data frame would have n rows and p columns.\n\n\n\nOur 100 people is considered a sample from some population, maybe the population of all UC Davis students. Our 25 elementary schools may have been chosen randomly among all such schools in the state.\nSince the data is considered to have been sampled randomly, the Xi are random. This randomness is at the heart of statistical inference, as you will see below.\nIn some cases, the sampled “population” may be more conceptual than real. Say we collect data on all UCD students. They might be considered to be a “sample” from the set of all such students, past, present and future. There may be issues here, e.g. change in student characteristics over time, so some caution must be exercized in applying statistical methods in such contexts.\n\n\n\nThis section is longer but involves the very essence of statistical inference. Some extra effort on the reader’s part here will pay large dividends.\nAround election times, one often here on TV newscasts the results of opinion polls, statements along the lines of “Candidate X is supported by 54.5% of the voters, according to a survey released today. The margin of error is 3.2%.” We’ll discuss the exact meaning of the margin of error later, but for now our focus is on why we need something like a margin of error, or more broadly, a standard error.\nSuppose the poll data had n = 1200, i.e. 1200 people were surveyed, say via random selection from a list of phone numbers. Let’s use Q to denote the proportion of people in our poll who say they will vote for Candidate X, so Q = 0.545 above.\nIf, hypothetically, the poll were to be conducted a second time, we would have a different set of 1200 people, and this time there might be, say, 51.7% of them saying they will vote for Candidate X, i.e. Q = 0.517. In other words, there is sampling variation in Q.\nSimilarly, say we are interested in the mean weight of all UCD students, based on a sample of 100. Our estimate of that population mean will be the average weight in our sample, denoted Xbar. Well, Xbar will be subject to sampling variation too; each different set of 100 students will have a different value of Xbar.\nIn general, let T be our estimate of some population quantity θ. E.g. in the weight example above, T would be Xbar, and θ would be the mean weight of students over the entire population. Is T probably pretty accurate, i.e. probably pretty close to the true (though unknown) value of θ? We reason as follows.\nWe just have our one sample, and hope that it yields a T value near θ, but we have to wonder: Is this sample representative? Might other samples have values of T that are closer to the true (but unknown) value of θ?\nKey point: If we know that T doesn’t vary much from one sample to another, that concern is addressed! If the sampling variability of T is small, then for most samples T will be near θ, so our value of T is likely near θ. No guarantees of course.\nSo, what measure do we use to describe the variability of T? We use the same measure that is used to describe the variability of any quantity–its standard deviation. And that’s what the standard error of T is–its standard deviation over all possible samples. Let’s denote it as se(T).\nBut wait a minute…that standard deviation is a population quantity, so we don’t know it anymore than we know the true value of θ. Have we hit a dead end here? It turns out that the answer is no. We can estimate that standard deviation too.\nThere is also a related question: What if most of the values of T are near each other, BUT not centered near θ? This is a question of (near-) unbiasedness of T. It’s beyond the scope of “bare minimum or less” tutorial, but see Lesson STDERRS of the fastStat tutorial.\n\n\n\nRecall the political poll example, and the notion of margin of error. Recall that we used Q to denote the proportion of Candidate X supporters in our sample? How does the margin of error relate to standard error? Actually, the relation is simple:\nmargin of error = 1.96 x se(Q)\nSo to get a range for q, the true population proportion of voters supporting X, we compute the interval\nQ ± 1.96 x se(Q)\nThis is then an approximate 95% confidence interval for q, the true population proportion of voters favoring Candidate X.\nWhat does it mean to say we are “95% confident” than q is in that interval? It simply means that 95% of all possible samples will yield intervals that contain the true q.\nOf course, we don’t know the situation for our particular sample. Instead, the above probability interpretation is analogous to, say, dice. If we roll 2 dice, the probability that their sum is at least 11 is 8/9. So if we roll 2 dice 900 times, about 800 of the roles will come out 11 or higher, so we say we “probably” will get a sum of at least 11.\nLessons CI and CIAPPROX in the fastStat tutorial go into the details.\n\n\n\nThese days machine learning is really in vogue. The basic goal is to preduct one variable, i.e. guess its value, from others. We might want to predict whether a loan applicant will pay off the loan, based on job status, previous credit history, etc. Or we may wish to predict whether someone has a certain disease, based on physical measurements, family history and so on.\nHere is the basic approach:\n\nTo predict some variable Y from X = t, the optimal rule is to guess Y to be the mean Y among all population entities have X = t.\n\nE.g. to predict someone’s weight from their height and age, say height 70 and age 28: Our guess is the mean weight of all people of height 70 and age 28. Note that this is a population quantity.\nThe issue is then:\n\nHow can we find subgroup means of this sort?\n\nThis is usually not so easy as it sounds. If, say, our sample (training data in machine learning parlance) consists of only n = 100 people, we may not have any people of height 70 and age 28, so we cannot find the desired mean directly from our data. Or, we might have just, say, 2 or 3 such people, hardly enough to get a reliable estimate of the mean.\nWhat can be done?\n\nWe could look at people in our sample whose height and weight are near 70 and 28, and find their average weight. That would be our estimate of the mean weight of all people in the population of height 70 and age 28. This is what many machine learning algorithms boil down to, especially the k-Nearest Neighbors and Random Forests methods.\nWe could assume that, viewed as a function of the two variables height and age, the quantity mean(weight for a given height and age) follows a certain mathematical formula, typically the linear model.\n\nIn our “bare minimum, even less” context here, we will focus on linear models. See the fastStat tutorial for machine learning methods, starting with Lesson PREDICT.\nImportant note: Linear and other prediction models can be used for more than prediction! A very common use is effect measurement. In a study of gender pay gap, we may posit that several variables affect wages, such as education, age, type of job and so on. We might use a linear model to gauge the effect of gender, in the presence of the other variables. We’ll do more on this later in the tutorial.\n\n\n\nWe assume a linear model, e.g. for predicting weight from height and age:\nmean weight = β0 + β1 height + β2 age\nThe βi are population values, which we estimate from the data. Call the estimates bi. Note that the bi are random, since they are calculated from our sample data, and thus have standard errors. For instance, b2, the estimated age coefficient, will vary from one sample to another; its standard error helps us assess whether our b2 is likely reasonably close to β2.\nHere is an example in R, using a dataset mlb on Major League Baseball players.\n&gt; z &lt;- lm(Weight ~ Height+Age,data=mlb)  # mlb dataset, MLB players\n&gt; summary(z)\n\ncoefficients:\n             estimate std. error t value pr(&gt;|t|)    \n(intercept) -187.6382    17.9447  -10.46  &lt; 2e-16 ***\nheight         4.9236     0.2344   21.00  &lt; 2e-16 ***\nage            0.9115     0.1257    7.25 8.25e-13 ***\n(Ignore all but the first column for now.)\nNow, what about our earlier question of what value we should use to predict the weight of a person who is 70 inches tall and is 28 years old? Remember, our model is\nmean weight = β0 + β1 height + β2 age\nso our prediction for the weight of such a person is\nβ0 + β1 70 + β2 28\nThe βi are known, so we replace them by our estimates:\nb0 + b1 70 + b2 28 = -187.64 + 4.92 x 70 + 0.91 x 28\nand that will be our predicted value. Actually, R does that for us:\n&gt; predict(z,data.frame(Height=70,Age=28))\n       1 \n182.5367 \nWe predict a weight of about 182 pounds.\nSee more in Lesson LIN of the fastStat tutorial.\n\n\n\nAccording to our above model, players of height 70 and age 28 have mean weight\nβ0 + β1 70 + β2 28\nFor players of that height but age 29, the figure is\nβ0 + β1 70 + β2 29\nSubtracting the age 28 expression from the age 29 one, we have\nmean weight (height 70, age 29) - mean weight (height 70, age 28) = β2\nThe other terms canceled out. In fact, they would have canceled out if we had compared 32-year-olds to those of age 33, etc. And furthermore, we see that this also would have been true for players of height 73.\nIn other words:\n\nThe average effect on weight of one extra year of age is β2.\n\nOur estimate of the unknown β2 is b2, which we found above to be 0.9115. So, mean weight among the players increases almost a pound per year of age, perhaps surprising for such a physically fit group.\nBut wait–might this be a sampling artifact? Let’s look at the standard error of this figure, shown in the above output to be 0.1257. This gives an approximate 95% confidence interval for the true population β2:\n0.9915 ± 1.96 x 0.1257 = (0.6651,1.1579)\nSo it does seem that there is a weight gain here, at least about 2/3 of a pound per year.\n\n\n\nOften a variable of interest is dichotomous. For instance, in predicting human weight from height and age, we might also have gender data. These are typically coded 1 and 0, say 1 for male, 0 for female. These variables are known as indicator variables. Our model of mean weight would then be\nmean weight = β0 + β1 height + β2 age + β3 Imale\nwhere Imale is equal to 1 for a man, 0 for a woman.\nMore in Lesson INDICATORS in the fastStat tutorial.\n\n\n\nWe saw above that linear models can be used not only for prediction but also for effect estimation. In fact, they may be used for fairer, more meaningful estimation, as will be shown in this section.\nLet’s first consider the famous UC Berkeley data on admissions to grad school. It was alleged that Berkeley was biased against female applicants, which seemed odd, so statistics professor Peter Bickel took a fresh look at the data.\nIt turned out to be true the over proportion\n(male applicants admitted) / (male applicants)\nwas greater than the corresponding fraction for females,\n(female applicants admitted) / (female applicants)\nBut it turned out that the overall lower rate for women was largely due to their applying to the more selective departments. Comparing admissions rates in individual departments, the seeming anomaly disappeared; male and female rates were similar in most departments, and in some the female rare was considerably higher.\nThe lesson to be learned is:\n\nRelations between two variables, in this case Admission and Gender, can be highly impacted by a third variable, Department here.\n\nThe Latin term ceteris paribus, “all else being equal,” applies here. In comparing male and female admissions rates, we must make sure that all other variables are equal, e.g. compare the male admissions rate in Department A to the female rate in Department A.\nLinear models are often used for ceteris paribus analyses. Let’s look at an analysis of a possible gender pay gap. The pef dataset gives census data in Silicon Valley, back in the year 2000. Six different tech job titles were present.\nHere is what the first six rows of the data look like:\n&gt; head(pef)\n       age     educ occ sex wageinc wkswrkd\n1 50.30082 zzzOther 102   2   75000      52\n2 41.10139 zzzOther 101   1   12300      20\n3 24.67374 zzzOther 102   2   15400      52\n4 50.19951 zzzOther 100   1       0      52\n5 51.18112 zzzOther 100   2     160       1\n6 57.70413 zzzOther 100   1       0       0\nA linear model was used, with mean income modeled in terms of age, education, job title, gender and number of weeks worked. The occupation column was converted to indicator variables. Here are the results:\n&gt; coef(lm(wageinc ~ .,data=pef))\n (Intercept)          age       educ16 educzzzOther       occ101\nocc102 \n -12669.3225     469.8373    7597.2521  -14167.3888    1838.5510\n13688.3012 \n      occ106       occ140       occ141         sex2      wkswrkd \n   1380.0254   11732.5837   10791.0158   -8595.5831    1323.1999 \nR created a new indicator variable here, sex2, which indicated female; its value is 1 for women, 0 for men. Here the gender variable is coded 1 for men, 2 for women.\nIn the same manner as where the age effect in the baseball player example could be assessed by the coefficient of the Age variable, the coefficient for sex2, -8595.5831, is our estimate of the gender pay gap: Comparing men and women of the same age, same education and so on, we found that women were paid about $8600 less than comparable men.\n\n\n\nThis is one of the tragedies of statistics: An old method has long been shown to be at worst misleading and at best noninformative, yet was so entrenched that it continues to be very widely used today. Here we give a brief description of the method, significance testing, and explain its pitfalls.\nThe method first sets a null hypothesis H0. In the gender pay gap example, we might have\nH0: βsex2 = 0\nHere H0 hypothesizes that there is no gender pay gap.\nThe decision is made on the basis of “innocent until proven guilty”: We believe H0p is true until and unless we find strong evidence to the contrary. Well, then, what constitutes “strong evidence”?\nThe answer is that we look at the quotient\nZ = bsex2 / (standard error of bsex2\nIf H0 were true, Z would be between -1.96 and 1.96 with probability 0.95. (Yes, these numbers do also appear in our section on confidence intervals, but the usage and interpretation will be different here.) Again, if H0 were true, that quotient would have only a 5% chance of being larger than 1.96 or less than -1.96. So, if the quotient does fall into that outside range, we say, “This would be unlikely under H0, so we will abandon our belief in H0.”\nSAT ex; don’t just see if CI contains 0; p-value “greed”"
  },
  {
    "objectID": "old/StatBareMin.html#coverage",
    "href": "old/StatBareMin.html#coverage",
    "title": "Statistics–the Bare Minimum (or even less)",
    "section": "",
    "text": "Main topics:\n\ncentral importance of sampling variation\nstandard errors; statistical inference\nprediction, especially with linear models\neffect estimation, confounding\n\nNotably missing:\n\nnormal, chi-square, F distributions\ninordinate emphasis of hypothesis testing and p-values"
  },
  {
    "objectID": "old/StatBareMin.html#data-notation",
    "href": "old/StatBareMin.html#data-notation",
    "title": "Statistics–the Bare Minimum (or even less)",
    "section": "",
    "text": "Say we have data on height, age and weight for 100 people. Then the fact that we have 3 variables is denoted p = 3. The number of people, 100 here, is denoted n = 100.\nOf course, data points need not be people. For instance, we may have data on a sample of n = 25 elementary schools in a US state, with the data on any school being the number of students and number of teachers. We’d have p = 2 here.\nBy the way, the proper statistical phrasing is that, say in the school example, is “We have a sample of size 25,” not “We have 25 samples.”\nWe’ll use X1 to denote the first data point in our sample. In the people example above, X1 denotes the height, age and weight of the first person in our sample. X2 is our second data point, say the numbers of students and teachers for the second school in the school example. X3, X4 and so on are defined similarly.\nStorage in a data frame would have n rows and p columns."
  },
  {
    "objectID": "old/StatBareMin.html#sampling-from-populations-conceptual-or-real",
    "href": "old/StatBareMin.html#sampling-from-populations-conceptual-or-real",
    "title": "Statistics–the Bare Minimum (or even less)",
    "section": "",
    "text": "Our 100 people is considered a sample from some population, maybe the population of all UC Davis students. Our 25 elementary schools may have been chosen randomly among all such schools in the state.\nSince the data is considered to have been sampled randomly, the Xi are random. This randomness is at the heart of statistical inference, as you will see below.\nIn some cases, the sampled “population” may be more conceptual than real. Say we collect data on all UCD students. They might be considered to be a “sample” from the set of all such students, past, present and future. There may be issues here, e.g. change in student characteristics over time, so some caution must be exercized in applying statistical methods in such contexts."
  },
  {
    "objectID": "old/StatBareMin.html#standard-errors",
    "href": "old/StatBareMin.html#standard-errors",
    "title": "Statistics–the Bare Minimum (or even less)",
    "section": "",
    "text": "This section is longer but involves the very essence of statistical inference. Some extra effort on the reader’s part here will pay large dividends.\nAround election times, one often here on TV newscasts the results of opinion polls, statements along the lines of “Candidate X is supported by 54.5% of the voters, according to a survey released today. The margin of error is 3.2%.” We’ll discuss the exact meaning of the margin of error later, but for now our focus is on why we need something like a margin of error, or more broadly, a standard error.\nSuppose the poll data had n = 1200, i.e. 1200 people were surveyed, say via random selection from a list of phone numbers. Let’s use Q to denote the proportion of people in our poll who say they will vote for Candidate X, so Q = 0.545 above.\nIf, hypothetically, the poll were to be conducted a second time, we would have a different set of 1200 people, and this time there might be, say, 51.7% of them saying they will vote for Candidate X, i.e. Q = 0.517. In other words, there is sampling variation in Q.\nSimilarly, say we are interested in the mean weight of all UCD students, based on a sample of 100. Our estimate of that population mean will be the average weight in our sample, denoted Xbar. Well, Xbar will be subject to sampling variation too; each different set of 100 students will have a different value of Xbar.\nIn general, let T be our estimate of some population quantity θ. E.g. in the weight example above, T would be Xbar, and θ would be the mean weight of students over the entire population. Is T probably pretty accurate, i.e. probably pretty close to the true (though unknown) value of θ? We reason as follows.\nWe just have our one sample, and hope that it yields a T value near θ, but we have to wonder: Is this sample representative? Might other samples have values of T that are closer to the true (but unknown) value of θ?\nKey point: If we know that T doesn’t vary much from one sample to another, that concern is addressed! If the sampling variability of T is small, then for most samples T will be near θ, so our value of T is likely near θ. No guarantees of course.\nSo, what measure do we use to describe the variability of T? We use the same measure that is used to describe the variability of any quantity–its standard deviation. And that’s what the standard error of T is–its standard deviation over all possible samples. Let’s denote it as se(T).\nBut wait a minute…that standard deviation is a population quantity, so we don’t know it anymore than we know the true value of θ. Have we hit a dead end here? It turns out that the answer is no. We can estimate that standard deviation too.\nThere is also a related question: What if most of the values of T are near each other, BUT not centered near θ? This is a question of (near-) unbiasedness of T. It’s beyond the scope of “bare minimum or less” tutorial, but see Lesson STDERRS of the fastStat tutorial."
  },
  {
    "objectID": "old/StatBareMin.html#confidence-intervals",
    "href": "old/StatBareMin.html#confidence-intervals",
    "title": "Statistics–the Bare Minimum (or even less)",
    "section": "",
    "text": "Recall the political poll example, and the notion of margin of error. Recall that we used Q to denote the proportion of Candidate X supporters in our sample? How does the margin of error relate to standard error? Actually, the relation is simple:\nmargin of error = 1.96 x se(Q)\nSo to get a range for q, the true population proportion of voters supporting X, we compute the interval\nQ ± 1.96 x se(Q)\nThis is then an approximate 95% confidence interval for q, the true population proportion of voters favoring Candidate X.\nWhat does it mean to say we are “95% confident” than q is in that interval? It simply means that 95% of all possible samples will yield intervals that contain the true q.\nOf course, we don’t know the situation for our particular sample. Instead, the above probability interpretation is analogous to, say, dice. If we roll 2 dice, the probability that their sum is at least 11 is 8/9. So if we roll 2 dice 900 times, about 800 of the roles will come out 11 or higher, so we say we “probably” will get a sum of at least 11.\nLessons CI and CIAPPROX in the fastStat tutorial go into the details."
  },
  {
    "objectID": "old/StatBareMin.html#prediction",
    "href": "old/StatBareMin.html#prediction",
    "title": "Statistics–the Bare Minimum (or even less)",
    "section": "",
    "text": "These days machine learning is really in vogue. The basic goal is to preduct one variable, i.e. guess its value, from others. We might want to predict whether a loan applicant will pay off the loan, based on job status, previous credit history, etc. Or we may wish to predict whether someone has a certain disease, based on physical measurements, family history and so on.\nHere is the basic approach:\n\nTo predict some variable Y from X = t, the optimal rule is to guess Y to be the mean Y among all population entities have X = t.\n\nE.g. to predict someone’s weight from their height and age, say height 70 and age 28: Our guess is the mean weight of all people of height 70 and age 28. Note that this is a population quantity.\nThe issue is then:\n\nHow can we find subgroup means of this sort?\n\nThis is usually not so easy as it sounds. If, say, our sample (training data in machine learning parlance) consists of only n = 100 people, we may not have any people of height 70 and age 28, so we cannot find the desired mean directly from our data. Or, we might have just, say, 2 or 3 such people, hardly enough to get a reliable estimate of the mean.\nWhat can be done?\n\nWe could look at people in our sample whose height and weight are near 70 and 28, and find their average weight. That would be our estimate of the mean weight of all people in the population of height 70 and age 28. This is what many machine learning algorithms boil down to, especially the k-Nearest Neighbors and Random Forests methods.\nWe could assume that, viewed as a function of the two variables height and age, the quantity mean(weight for a given height and age) follows a certain mathematical formula, typically the linear model.\n\nIn our “bare minimum, even less” context here, we will focus on linear models. See the fastStat tutorial for machine learning methods, starting with Lesson PREDICT.\nImportant note: Linear and other prediction models can be used for more than prediction! A very common use is effect measurement. In a study of gender pay gap, we may posit that several variables affect wages, such as education, age, type of job and so on. We might use a linear model to gauge the effect of gender, in the presence of the other variables. We’ll do more on this later in the tutorial."
  },
  {
    "objectID": "old/StatBareMin.html#linear-models",
    "href": "old/StatBareMin.html#linear-models",
    "title": "Statistics–the Bare Minimum (or even less)",
    "section": "",
    "text": "We assume a linear model, e.g. for predicting weight from height and age:\nmean weight = β0 + β1 height + β2 age\nThe βi are population values, which we estimate from the data. Call the estimates bi. Note that the bi are random, since they are calculated from our sample data, and thus have standard errors. For instance, b2, the estimated age coefficient, will vary from one sample to another; its standard error helps us assess whether our b2 is likely reasonably close to β2.\nHere is an example in R, using a dataset mlb on Major League Baseball players.\n&gt; z &lt;- lm(Weight ~ Height+Age,data=mlb)  # mlb dataset, MLB players\n&gt; summary(z)\n\ncoefficients:\n             estimate std. error t value pr(&gt;|t|)    \n(intercept) -187.6382    17.9447  -10.46  &lt; 2e-16 ***\nheight         4.9236     0.2344   21.00  &lt; 2e-16 ***\nage            0.9115     0.1257    7.25 8.25e-13 ***\n(Ignore all but the first column for now.)\nNow, what about our earlier question of what value we should use to predict the weight of a person who is 70 inches tall and is 28 years old? Remember, our model is\nmean weight = β0 + β1 height + β2 age\nso our prediction for the weight of such a person is\nβ0 + β1 70 + β2 28\nThe βi are known, so we replace them by our estimates:\nb0 + b1 70 + b2 28 = -187.64 + 4.92 x 70 + 0.91 x 28\nand that will be our predicted value. Actually, R does that for us:\n&gt; predict(z,data.frame(Height=70,Age=28))\n       1 \n182.5367 \nWe predict a weight of about 182 pounds.\nSee more in Lesson LIN of the fastStat tutorial."
  },
  {
    "objectID": "old/StatBareMin.html#estimation-of-effects",
    "href": "old/StatBareMin.html#estimation-of-effects",
    "title": "Statistics–the Bare Minimum (or even less)",
    "section": "",
    "text": "According to our above model, players of height 70 and age 28 have mean weight\nβ0 + β1 70 + β2 28\nFor players of that height but age 29, the figure is\nβ0 + β1 70 + β2 29\nSubtracting the age 28 expression from the age 29 one, we have\nmean weight (height 70, age 29) - mean weight (height 70, age 28) = β2\nThe other terms canceled out. In fact, they would have canceled out if we had compared 32-year-olds to those of age 33, etc. And furthermore, we see that this also would have been true for players of height 73.\nIn other words:\n\nThe average effect on weight of one extra year of age is β2.\n\nOur estimate of the unknown β2 is b2, which we found above to be 0.9115. So, mean weight among the players increases almost a pound per year of age, perhaps surprising for such a physically fit group.\nBut wait–might this be a sampling artifact? Let’s look at the standard error of this figure, shown in the above output to be 0.1257. This gives an approximate 95% confidence interval for the true population β2:\n0.9915 ± 1.96 x 0.1257 = (0.6651,1.1579)\nSo it does seem that there is a weight gain here, at least about 2/3 of a pound per year."
  },
  {
    "objectID": "old/StatBareMin.html#indicator-variables",
    "href": "old/StatBareMin.html#indicator-variables",
    "title": "Statistics–the Bare Minimum (or even less)",
    "section": "",
    "text": "Often a variable of interest is dichotomous. For instance, in predicting human weight from height and age, we might also have gender data. These are typically coded 1 and 0, say 1 for male, 0 for female. These variables are known as indicator variables. Our model of mean weight would then be\nmean weight = β0 + β1 height + β2 age + β3 Imale\nwhere Imale is equal to 1 for a man, 0 for a woman.\nMore in Lesson INDICATORS in the fastStat tutorial."
  },
  {
    "objectID": "old/StatBareMin.html#ceteris-paribus-comparisons",
    "href": "old/StatBareMin.html#ceteris-paribus-comparisons",
    "title": "Statistics–the Bare Minimum (or even less)",
    "section": "",
    "text": "We saw above that linear models can be used not only for prediction but also for effect estimation. In fact, they may be used for fairer, more meaningful estimation, as will be shown in this section.\nLet’s first consider the famous UC Berkeley data on admissions to grad school. It was alleged that Berkeley was biased against female applicants, which seemed odd, so statistics professor Peter Bickel took a fresh look at the data.\nIt turned out to be true the over proportion\n(male applicants admitted) / (male applicants)\nwas greater than the corresponding fraction for females,\n(female applicants admitted) / (female applicants)\nBut it turned out that the overall lower rate for women was largely due to their applying to the more selective departments. Comparing admissions rates in individual departments, the seeming anomaly disappeared; male and female rates were similar in most departments, and in some the female rare was considerably higher.\nThe lesson to be learned is:\n\nRelations between two variables, in this case Admission and Gender, can be highly impacted by a third variable, Department here.\n\nThe Latin term ceteris paribus, “all else being equal,” applies here. In comparing male and female admissions rates, we must make sure that all other variables are equal, e.g. compare the male admissions rate in Department A to the female rate in Department A.\nLinear models are often used for ceteris paribus analyses. Let’s look at an analysis of a possible gender pay gap. The pef dataset gives census data in Silicon Valley, back in the year 2000. Six different tech job titles were present.\nHere is what the first six rows of the data look like:\n&gt; head(pef)\n       age     educ occ sex wageinc wkswrkd\n1 50.30082 zzzOther 102   2   75000      52\n2 41.10139 zzzOther 101   1   12300      20\n3 24.67374 zzzOther 102   2   15400      52\n4 50.19951 zzzOther 100   1       0      52\n5 51.18112 zzzOther 100   2     160       1\n6 57.70413 zzzOther 100   1       0       0\nA linear model was used, with mean income modeled in terms of age, education, job title, gender and number of weeks worked. The occupation column was converted to indicator variables. Here are the results:\n&gt; coef(lm(wageinc ~ .,data=pef))\n (Intercept)          age       educ16 educzzzOther       occ101\nocc102 \n -12669.3225     469.8373    7597.2521  -14167.3888    1838.5510\n13688.3012 \n      occ106       occ140       occ141         sex2      wkswrkd \n   1380.0254   11732.5837   10791.0158   -8595.5831    1323.1999 \nR created a new indicator variable here, sex2, which indicated female; its value is 1 for women, 0 for men. Here the gender variable is coded 1 for men, 2 for women.\nIn the same manner as where the age effect in the baseball player example could be assessed by the coefficient of the Age variable, the coefficient for sex2, -8595.5831, is our estimate of the gender pay gap: Comparing men and women of the same age, same education and so on, we found that women were paid about $8600 less than comparable men."
  },
  {
    "objectID": "old/StatBareMin.html#significance-testing-and-p-values",
    "href": "old/StatBareMin.html#significance-testing-and-p-values",
    "title": "Statistics–the Bare Minimum (or even less)",
    "section": "",
    "text": "This is one of the tragedies of statistics: An old method has long been shown to be at worst misleading and at best noninformative, yet was so entrenched that it continues to be very widely used today. Here we give a brief description of the method, significance testing, and explain its pitfalls.\nThe method first sets a null hypothesis H0. In the gender pay gap example, we might have\nH0: βsex2 = 0\nHere H0 hypothesizes that there is no gender pay gap.\nThe decision is made on the basis of “innocent until proven guilty”: We believe H0p is true until and unless we find strong evidence to the contrary. Well, then, what constitutes “strong evidence”?\nThe answer is that we look at the quotient\nZ = bsex2 / (standard error of bsex2\nIf H0 were true, Z would be between -1.96 and 1.96 with probability 0.95. (Yes, these numbers do also appear in our section on confidence intervals, but the usage and interpretation will be different here.) Again, if H0 were true, that quotient would have only a 5% chance of being larger than 1.96 or less than -1.96. So, if the quotient does fall into that outside range, we say, “This would be unlikely under H0, so we will abandon our belief in H0.”\nSAT ex; don’t just see if CI contains 0; p-value “greed”"
  }
]